---
title: "Data Mining Homework 4"
author: "Hannah Jones"
date: "5/3/2021"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE)

library(tidyverse)
library(ggplot2)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(scales)
library(lubridate)
library(randomForest)
library(pdp)
library(LICORS)  # for kmeans++
library(foreach)
library(readr)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(igraph)
library(data.table)
library(tm) 
library(slam)
library(proxy)
library(reshape2)
library(cluster)
library(HSAUR)
library(fpc)
```

## Clustering and PCA: Wine Color and Quality 

The wine dataset details wine color, quality, and 11 chemical attributes.  Using these variables, I performed principle component analysis, compressing all the variables down into 5 principle components.  Once I examined these components, I found that only 1 component (PC1) was very powerful in its correlation with color, and no principle component consistently is correlated with quality, so I reduced the PCA to just 2 components.  Below is a plot of the two principle components, with the color of each data point denoting the color of the wine in the first graph (red = 1, white = 0), and color denoting wine quality in the second.  

```{r}
#PROBLEM 1
wine = read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv')

wine<- wine%>%
  mutate(color = ifelse(color == 'red', 1, 0))

wine_trim = select(wine, -c(color, quality))

wine_trim = scale(wine_trim, center=TRUE, scale=TRUE)

mu = attr(wine_trim,"scaled:center") # mean
sigma = attr(wine_trim,"scaled:scale")
# Compare these random projections to the first PC
wine_pca = prcomp(wine_trim, rank=2)

loadings_wine = wine_pca$rotation
scores_wine = wine_pca$x
summary(wine_pca)

qplot(scores_wine[,1], scores_wine[,2], xlab='Component 1', ylab='Component 2', color = wine$color)
qplot(scores_wine[,1], scores_wine[,2], xlab='Component 1', ylab='Component 2', color = wine$quality)

```

Clearly, component 1 is highly correlated with the wine color.  Neither component is correlated with quality.  I then combine the PC1 and PC2 variables with the dataset for each wine, and build a random forest model of wine color regressed on these components on a training set.  The resulting RMSE from testing the model on a test set against actual wine color is below (~0.1). Additionally, the variable importance plot below confirms that PC1 is most significant in predicting wine color.   

```{r warning = FALSE}

#Predict using PCA

wine_combined = data.frame(wine, wine_pca$x)

train_frac = 0.8
N = nrow(wine_combined)
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
wine_train = wine_combined[train_ind,]
wine_test = wine_combined[-train_ind,]

forest_color = randomForest(color ~ PC1 + PC2, data = wine_train)

yhat_forest_color = predict(forest_color, wine_test)
mean((yhat_forest_color - wine_test$color)^2) %>% sqrt

#no_trees = plot(forest_color)
varImpPlot(forest_color)
```

I then tested out the predictive power of this random forest model for predicting wine quality. The RMSE and variable importance plot is below.  PC2 is most important in predicting wine quality, but PC1 is also significant.

```{r}

forest_quality = randomForest(quality ~ PC1 + PC2,
                            data = wine_train)

yhat_forest_qual = predict(forest_quality, wine_test)
mean((yhat_forest_qual - wine_test$quality)^2) %>% sqrt

#plot(forest_quality)
varImpPlot(forest_quality)

```

### Clustering for Wine Color and Quality

I also ran k-means clustering to predict wine color by removing the color and quality variables, and splitting the data into 2 clusters based on the other 11 chemical elements.  To see how effectively the clusters captured color, I attached the cluster assignment to the data and observed that cluster 1 captures mostly red wines, and cluster 2 captures mostly white wines.  I then looked at the RMSE to see how well the clusters align with the actual wine color.  The kmeans-method captured wine color effectively with an RMSE of ~0.11.

```{r}

#use clustering - KMEANS

wine_train = subset(wine, select = -c(color, quality))

wine_train_scaled = scale(wine_train, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(wine_train_scaled,"scaled:center")
sigma = attr(wine_train_scaled,"scaled:scale")

# Run k-means with 2 clusters and 25 starts
clust1 = kmeans(wine_train_scaled, 2, nstart=25)

wine_clustered = data.frame(wine, clust1$cluster)

wine_clustered <- wine_clustered%>%
  mutate(predicted = ifelse(clust1.cluster == 1, 1, 0))

#RMSE of clustering
mean((wine_clustered$predicted - wine_clustered$color)^2) %>% sqrt

# What are the clusters?
#clust1$center  # not super helpful
#clust1$center[1,]*sigma + mu
#clust1$center[2,]*sigma + mu

```

### Conclusion

I recommend using the PCA method of predicting wine color.  Kmeans clustering also does a decnt job of predicting wine color, just is not quite as effective as PCA.  I found that there is no reliable way to predict the wine quality score, which is understandable as quality is objective and dictated by a diverse group of people and palates.  


## Market Segmentation: Twitter Marketing

HOW CHOOSE # OF CLUSTERS

In order to streamline and sharpen the marketing efforts of NutrientH2O, we have assembled a dataset of tweet categorizations for the brand's followers.  After analyzing this data, we have a few insights that may lead to more focused, productive marketing.

```{r}
social_marketing <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')

social_marketing$ID <- seq.int(nrow(social_marketing))

#cut highly correlated

social_cut = select(social_marketing, -c(X, chatter, spam, adult, photo_sharing, health_nutrition))

#center and scale
X = scale(social_cut, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

#ch metric
k_grid = seq(2, 30, by=1)
N = nrow(X)
CH_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  W = cluster_k$tot.withinss
  B = cluster_k$betweenss
  CH = (B/W)*((N-k)/(k-1))
  CH 
}

plot(k_grid, CH_grid, las=1)

#choose k = 7

clust1 = kmeanspp(X, 5, nstart=25)

clust1$size

plotcluster(social_cut, clust1$cluster)

#one large cluster, rest same size-- lets learn more
cluster1 <- clust1$center[1,]*sigma + mu
cluster2 <- clust1$center[2,]*sigma + mu
cluster3 <- clust1$center[3,]*sigma + mu
cluster4 <- clust1$center[4,]*sigma + mu
cluster5 <- clust1$center[5,]*sigma + mu


social_cluster <- cbind(cluster1, cluster2, cluster3, cluster4, cluster5)
social_cluster = as.data.frame(social_cluster)
social_cluster$type <- row.names(social_cluster)
#remove ID
social_cluster = social_cluster[-32,]

ggplot(social_cluster, aes(x =reorder(type, -cluster1) , y=cluster1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values")

ggplot(social_cluster, aes(x =reorder(type, -cluster2) , y=cluster2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")

ggplot(social_cluster, aes(x =reorder(type, -cluster3) , y=cluster3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")

ggplot(social_cluster, aes(x =reorder(type, -cluster4) , y=cluster4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")

ggplot(social_cluster, aes(x =reorder(type, -cluster5) , y=cluster5)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 5",
        x ="Category", y = "Cluster centre values")

```

After some experimentation, I used kmeans++ to sort the tweet data into clusters.  After experimenting with various numbers of clusters, I chose to separate the data into 5 clusters.  These clusters are different sizes, but capture very specific interest groups. One cluster is by far the largest, and the other 4 are smaller in comparison, but capture more specific interests.  The five clusters can be summarized by their key topics below.

#### Cluster topics: 
Cluster A, "say it with a tweet": Current Events, Personal Fitness, Shopping
Cluster B, "college kids": College, Online Gaming
Cluster C, "pinterest crowd": Cooking, Fashion, Beauty
Cluster D, "southern parents": Sports, religion, food, parenting
Cluster E, "business crowd": politics, travel, news

I have created a nickname for each cluster based on the topics.  The largest cluster, Cluster A (5207 users), is most likely the cluster of people who tweet about anything and everything.  This cluster may be difficult to hone in on as it is more difficult to target specifically.  However, the other 4 clusters are very specific and easily targetable.  

The "college kids" (455 users) cluster tweets about gaming and their colleges, as well as about sports.  Targeting this crowd with University-specific campaigns may be effective.

The "pinterest crowd" cluster (639 users) tweets often about cooking, fashion and beauty.  These three topics usually draw in a crowd that would be a heavy user of the platform Pinterest which enables users to easily find recipes, clothes, and cooking techniques, among other things.  This segment could be targeted through partnering with specific brands or going the "influencer" route and paying for instagram influencers to promote product.

The "southern parents"  (830 users) cluster tweets often about sports, religion, food and parenting.  This cluster can be targeted through ads during sporting events or by appealing to the benefits of the product for children or for energy needed when raising children.

The final cluster is the "business crowd" (751 users) who tweet about politics, travel and news.  This segment can be targeted through newspapers, travel lounges, or other news outlets.

Each cluster can be grown by using a targeted, thoughtful approach to marketing.





## Association Rules for Grocery Purchases

Using data from ~9800 grocery baskets, I have created a set of rules that reflect buying behavior.  After playing with the support and confidence levels, I chose to create rules based on a support level of > 0.0025, confidence of over 0.5, and with a maximum cart size of 8 items.  This specification created 645 'rules'.  After a lot of playing around with the confidence and support levels, I chose these levels to see a variety of relatively likely combinations, to then trim down into more likely rules.  The support level indicates the proportion of times any combination of items occurred in the dataset.  A support level of at least 0.0025 means that at the bare minimum, any rule must have occurred in the cart 25 times.  The confidence level captures the probability the right hand side groceries are in the cart, given the left hand side groceries are in the cart.  I felt a 50% confidence was a sufficient floor to work from.  Finally, I made the minimum length 1 item in order to capture in highly likely, single item purchases.

After creating this set of rules, I trimmed the rules down to only include rules with confidence over 50%.  Below is the plot of the full set of 171 rules.   The lift range on the graph details the increase in probability that the right hand side occurs, given the left hand side occurs.

```{r echo = FALSE, warning = FALSE}

groceries <- read_lines('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt', skip = 0, n_max = -1L)

groceries <- as.data.frame(groceries)

lists <- strsplit(groceries$groceries, split = ",")
all_lists = lapply(lists, unique)
listtrans = as(all_lists, "transactions")

groceryrules2 = apriori(listtrans, 
                     parameter=list(support=.0025, confidence=.25, minlen=1))

#arules::inspect(subset(groceryrules2, lift > 4))

plot(groceryrules2)

```


Using the trimmed subset with confidence > 0.6 and support > 0.005 yields the 22 rules below.  Notice that the lift for these rules ranges from ~2-3.  These rules reflect the frequency with which customers purchase whole milk and vegetables. It seems that these two items are the only ones that can be predicted with high confidence and support, likely due the frequency with which they are purchased.    

```{r}

sub1 = subset(groceryrules2, subset=confidence > 0.6 & support > 0.005)
arules::inspect(sub1)
plot(sub1, method='graph')


```

If we loosen the rules and only require that the lift be greater than 15, we find these rules below.  Sorting rules based on lift instead of confidence demonstrates clusters of items that are frequently bought together, even if they do not occur enough to result in high confidence or support.  The clusters below can be loosely interpreted as an alcohol cluster, a snack cluster, a baking cluster, a sandwich cluster, and other non-specific clusters.

```{r}
groceryrules3 = apriori(listtrans, 
                     parameter=list(support=.001, confidence=.1, maxlen=8))

sub2 = subset(groceryrules3, subset=lift>15)
arules::inspect(sub2)
plot(sub2, method='graph')

```

## Author Attribution

Fo rthis problem, I used text from 50 authors with 50 articles each to predict authorship based on words used.  In order to do this, I first read and cleaned all of the file names from the directory source.  I then read in the text from every article and stored in in a large corpus.  In order to standardize the text, I removed punctuation, capitalization and whitespace, as well as common words using the "SMART" dictionary.

Once all of the training and testing data was processed, I created a Document Term Matrix (DTM) for both the testing and training sets. 



```{r}

train_dir = Sys.glob(paths = '/Users/hannahjones/Desktop/ReutersC50/C50train/*')
train_dir = train_dir[c(1:50)]
read_files = NULL
labels = NULL
for(writer in train_dir){
  author = substring(writer, first = 1)
  article = Sys.glob(paste0(writer,'/*.txt'))
  read_files = append(read_files, article)
  labels = append(labels, rep(author, length(article)) )
}


readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

## create a text mining 'corpus' with: 
documents_raw = Corpus(DirSource(train_dir))

## Some pre-processing/tokenization steps.

train_docs <- documents_raw %>%
  tm_map(., content_transformer(tolower)) %>% 
  tm_map(., content_transformer(removeNumbers)) %>% 
  tm_map(., content_transformer(removeNumbers)) %>% 
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))


## create a doc-term-matrix, 2500 docs, 31423 terms
DTM_train = DocumentTermMatrix(train_docs)

## Finally, drop those terms that only occur in one or two documents, 3076 terms
DTM_train2 = removeSparseTerms(DTM_train, 0.96)
DTM_train2
```

The training DTM details are above.  I have trimmed the words that are only included a few times in order to eliminate noise.  We are left with ~850 words, down from ~31,000.  I repeat the same process of creating the DTM for the test set of data, except instead of trimming sparse words, I go ahead and set the limitation that all words in the test DTM must also be in the training DTM.

```{r}
# Data frame of 2500 variables and 641 variables (The first matrix is completed)
DF_train <- data.frame(as.matrix(DTM_train2), stringsAsFactors=FALSE)

#Clean the label names from before
author_names = labels %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=1) } %>%  
  { lapply(., paste0, collapse = '') } %>%
  unlist

#rename
#all_files = lapply(read_files, readerPlain)
#names(all_files) = author_names

author_names = as.data.frame(author_names)


#repeat all for test directory

test_dir = Sys.glob(paths = '/Users/hannahjones/Desktop/ReutersC50/C50train/*')
test_dir = test_dir[c(1:50)]
read_files1 = NULL
labels1 = NULL

for(writer1 in test_dir){
  author1 = substring(writer1, first = 1)
  article1 = Sys.glob(paste0(writer1,'/*.txt'))
  read_files1 = append(read_files1, article1)
  labels1 = append(labels1, rep(author1, length(article1)) )
}

author_names1 = labels1 %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=1) } %>%  
  { lapply(., paste0, collapse = '') } %>%
  unlist

## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
documents_raw1 = Corpus(DirSource(test_dir))

## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus

test_docs= documents_raw1%>% tm_map(., content_transformer(tolower)) %>% 
  tm_map(., content_transformer(removeNumbers)) %>% 
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))

## create a doc-term-matrix
DTM_test2 = DocumentTermMatrix(test_docs, control = list(dictionary = Terms(DTM_train2)))
DTM_test2
DF_test<- data.frame(as.matrix(DTM_test2), stringsAsFactors=FALSE)

```

Above are the details for the testing set which matches the training set.  

Next, I remove any columns in the testing or training sets that contain no values (no words in articles), and I confirm that all columns match between testing and training.


```{r}

#remove zero columns
training_df1<-DF_train[,which(colSums(DF_train) != 0)] 
testing_df1<-DF_test[,which(colSums(DF_test) != 0)]

#only keep matching columns
testing_df1 = testing_df1[,intersect(colnames(testing_df1),colnames(training_df1))]
training_df1 = training_df1[,intersect(colnames(testing_df1),colnames(training_df1))]
```

### PCA

In order to try prediction models, I first use PCA to create principle components to use as variables in prediction.  I create principle components using the training data, and then predict the PCA values onto the testing data.  I then look at the cumulative variance explained versus the number of principle components in order to choose a number of principle components that explains about 75% of the variance. This occurs at PC263, so I will move forward using the first 263 principle components.  The chart below shows the sumulative sum of the variance versus the number of principle components.

```{r echo = FALSE }
####
# Dimensionality reduction
####

mod_pca = prcomp(training_df1,scale=TRUE)
pred_pca=predict(mod_pca,newdata = testing_df1)

#plot(mod_pca, type = 'line') 
#plot(mod_pca)

var <- apply(mod_pca$x, 2, var)  
prop <- var / sum(var)
#cumsum(prop) # 75% of variance explained by PC 1 - 263
plot(cumsum(mod_pca$sdev^2/sum(mod_pca$sdev^2)))
```

Once I summarized my training data using principle component analysis, I created a new training dataset with only the principle components, and author names.  I also created a testing dataset with the scaled predicted principle components, and author names.  I then ran a random forest model to predict author using the principle components in the training set.  Using this model, I predicted the outcomes on the testing set, and looked at the 'hitrate' (percent correct predictions).
```{r}
train_author = data.frame(mod_pca$x[,1:263])
train_author['author']=author_names
train_load = mod_pca$rotation[,1:263]

test_author_pre <- scale(testing_df1) %*% train_load
test_author <- as.data.frame(test_author_pre)
test_author['author']=author_names1

# run a random forest using PCA variables in train_author

train_author$author = factor(train_author$author) 

author_forest = randomForest(author ~ .,
                              data = train_author, importance = TRUE)

yhat_author = predict(author_forest, test_author)

comp_table<-as.data.frame(table(yhat_author,as.factor(test_author$author)))
predicted<-yhat_author
actual<-as.factor(test_author$author)
comp_table<-as.data.frame(cbind(actual,predicted))
comp_table$flag<-ifelse(comp_table$actual==comp_table$predicted,1,0)
sum(comp_table$flag)
sum(comp_table$flag)*100/nrow(comp_table)
```

The two numbers above reflect the number of correct predictions out of 2500, and the percentage correct.  This model runs at about 50% accuracy.  

### KNN Prediction

Next, I will look at using a KNN model to see if I can improve the accuracy.  The KNN model will predict the author on the testing set using nearest neighbors prediction using k=5.

```{r}
#use KNN-- worse

train.X = subset(train_author, select = -c(author))
test.X = subset(test_author, select=-c(author))
train.author=as.factor(train_author$author)
test.author=as.factor(test_author$author)
library(class)
set.seed(1)
knn_pred=knn(train.X,test.X,train.author,k=5)
temp_knn=as.data.frame(cbind(knn_pred,test.author))
temp_knn_flag<-ifelse(as.integer(knn_pred)==as.integer(test.author),1,0)
sum(temp_knn_flag)
sum(temp_knn_flag)*100/nrow(temp_knn)
```

As shown above, the KNN model works about half as well as the random forest.  I will opt to use the Random Forest model.

### Conclusion

In general, I would have hoped for higher accuracy using one of these models.  However, the random forest model performed fairly well and would likely 




